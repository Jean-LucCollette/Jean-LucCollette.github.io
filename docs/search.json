[
  {
    "objectID": "Miniproject4.html",
    "href": "Miniproject4.html",
    "title": "Mini Project 4",
    "section": "",
    "text": "Jeopardy! is a long-running game show with a reversed format in which an answer is given first as a clue where the contestant has to come up with a question to that answer. The show is at times dubbed as America’s Favorite Quiz Show. It has run for over six-decades with over 8,000 episodes making it one of the longest-running game shows in television history.\nAmong the record holders, Ken Jennings holds the record for the longest winning streak at 74 games, earning over 2.5 million during regular play. James Holzhauer, a professional sports gambler, set records for single-game winnings, raking in 131,127 in one match and earning over $2.4 million in just 32 games.\nHolzhauer is also known for using a highly analytical and statistical approach to how to play the game show. He studied question patterns, clue values, and how to find Daily Doubles effectively. This led to an untraditional way of playing jeopardy where he would ask for questions randomly around the board instead of column-by-column.\nIn this project, I will try to analyze historical Jeopardy Questions to see if the text itself has any patterns among nearly 80% of questions before 2011. Does length of question differ from normal to double jeopardy to final jeopardy? Are easier questions different sentimentally than harder ones? Lets explore!\nThis data was sourced from this reddit post\nI found this jeopardy data from this list of text data sets\nMy github repository can be found on the home page\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidytext)\nlibrary(textdata)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(wordcloud2)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nlibrary(ggthemes)\nlibrary(gutenbergr)\nlibrary(RCurl)\n\n\nAttaching package: 'RCurl'\n\nThe following object is masked from 'package:tidyr':\n\n    complete\n\n\n\nlibrary(readr)\n# load data\nJEOPARDY_CSV &lt;- read_csv(\"~/RStudioProjectGit/Jean-LucCollette.github.io/JEOPARDY_CSV.csv\")\n\nRows: 216930 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Air Date, Round, Category, Value, Question, Answer\ndbl (1): Show Number\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# tidy data a bit (remove image questions)\njeopardy &lt;- JEOPARDY_CSV |&gt;\n  mutate(`Air Date` = mdy(`Air Date`)) |&gt;\n  filter(!str_detect(Question, \"href\"),\n         !str_detect(Question, \"_\")) # 1st regular expression\n\nhead(jeopardy)\n\n# A tibble: 6 × 7\n  `Show Number` `Air Date` Round     Category              Value Question Answer\n          &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;     &lt;chr&gt;                 &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; \n1          4680 2004-12-31 Jeopardy! HISTORY               $200  \"For th… Coper…\n2          4680 2004-12-31 Jeopardy! ESPN's TOP 10 ALL-TI… $200  \"No. 2:… Jim T…\n3          4680 2004-12-31 Jeopardy! EVERYBODY TALKS ABOU… $200  \"The ci… Arizo…\n4          4680 2004-12-31 Jeopardy! THE COMPANY LINE      $200  \"In 196… McDon…\n5          4680 2004-12-31 Jeopardy! EPITAPHS & TRIBUTES   $200  \"Signer… John …\n6          4680 2004-12-31 Jeopardy! 3-LETTER WORDS        $200  \"In the… the a…\n\n# have a word column instead of by long question\njeopardy_tidy &lt;- jeopardy |&gt;\n  unnest_tokens(output = word, input = Question) |&gt; # \n  mutate(Value = as.numeric(gsub(\"\\\\$\", \"\", Value)))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `Value = as.numeric(gsub(\"\\\\$\", \"\", Value))`.\nCaused by warning:\n! NAs introduced by coercion\n\n# lower case\njeopardy_tidy &lt;- jeopardy_tidy |&gt;\n  mutate(word = str_to_lower(word)) # 1st str_function\n\nhead(jeopardy_tidy)\n\n# A tibble: 6 × 7\n  `Show Number` `Air Date` Round     Category Value Answer     word \n          &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;\n1          4680 2004-12-31 Jeopardy! HISTORY    200 Copernicus for  \n2          4680 2004-12-31 Jeopardy! HISTORY    200 Copernicus the  \n3          4680 2004-12-31 Jeopardy! HISTORY    200 Copernicus last \n4          4680 2004-12-31 Jeopardy! HISTORY    200 Copernicus 8    \n5          4680 2004-12-31 Jeopardy! HISTORY    200 Copernicus years\n6          4680 2004-12-31 Jeopardy! HISTORY    200 Copernicus of   \n\n\n\nSentiment by question value\nHere is a graph of average word sentiment by question value for each. We can see that they are very similar but do vary between almost zero sentiment and higher values. This is likely influenced by “Daily Double” Questions where contestants can answer and bet how much they would like to wager to win before seeing the questions. Still, all of the sentiments are above or around zero showing that the majority of Jeopardy questions have positive sentiment with perhaps a slight decrease as questions get harder.\n\n# Graph 1\n\n#sentiments\nafinn_sentiments &lt;- get_sentiments(lexicon = \"afinn\")\n\n\n1# plot for sentiment\n\n[1] 1\n\njeopardy_tidy |&gt;\n  filter(Round != \"Final Jeopardy!\",\n         Round != \"Tiebreaker\") |&gt;\n  inner_join(afinn_sentiments) |&gt;\n  count(Round, index = Value, value) |&gt;\n  mutate(sentiment = value*n) |&gt;\n  group_by(index) |&gt;\n  summarize(Round = Round,\n            sentiment = mean(sentiment),\n            Question_Value = index) |&gt;\n  ggplot(aes(x = Question_Value, y = sentiment, color = Round)) +\n    geom_point() +\n    facet_wrap(~Round) +\n  labs(title = \"Jeopardy! Round average word sentiment\",\n       y = \"Sentiment\",\n       x = \"Question Value\",\n       color = \"Round\")\n\nJoining with `by = join_by(word)`\n\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n`summarise()` has grouped output by 'index'. You can override using the\n`.groups` argument.\n\n\nWarning: Removed 20 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nMost Distinguished Words by Round\nBy round, here is a chart of the most unique words by Round of Jeopardy! by using the TF-IDF statistic. It is clear from these graphs that regular Jeopardy! has the least unique values given closer statistics to 0, while Double Jeopardy! and Final Jeopardy! have more distinct words. This may be due to Final Jeopardy! having less questions overall, but still shows that questions may get slightly harder as the game goes on.\n\n# counting the words and removing \"___\"\nwordcount_jeopardy &lt;- jeopardy_tidy |&gt;\n  mutate(word = str_to_title(word)) |&gt; # 2nd str_function\n   filter(Round != \"Tiebreaker\",\n          !str_detect(word, \"\\\\_\"),\n          !str_detect(word, \"__\"))|&gt;\n  count(Round, word) |&gt;\n  anti_join(stop_words) |&gt;\n  group_by(Round)\n\nJoining with `by = join_by(word)`\n\n# making TF_IDF info\njeopardy_tidy_tf &lt;- wordcount_jeopardy |&gt;\n  bind_tf_idf(word, Round, n) |&gt;\n  arrange(-tf_idf)\n\n\n# Graph 2, text analysis 1\n\njeopardy_tidy_tf |&gt;\n  group_by(Round) |&gt;\n  arrange(desc(tf_idf)) |&gt;\n  slice_max(tf_idf, n = 10) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = fct_reorder(word, tf_idf), y = tf_idf, fill = Round)) +\n    geom_col(show.legend = FALSE) +\n    coord_flip() +\n    facet_wrap(~Round, scales = \"free\") +\n  labs(title = \"Most unique words by Round\",\n       y = \"TF-IDF statistic\",\n       x = \"Top 10 words\")\n\n\n\n\n\n\n\n\n\n\nPairs of Words\nWe can do the same thing with pairs of words by round to see what is most common. Here we see the most common types of questions asked in jeopardy contain “Also called” and “you might” which give scenarios for the answers you are given. Final Jeopardy has some more random combinations. There is data from nearly 6000 shows, of which most contain final jeopardy, so these questions are likely harder and have a different average format to the first two rounds. Still, for every final jeopardy question, there are 30 of each of Double Jeopardy and Jeopardy.\n\n# 2-word tendencies by round\njeopardy2word &lt;- jeopardy |&gt;\n     filter(Round != \"Tiebreaker\",\n          !str_detect(Question, \"\\\\_\"), # 2nd regular expression\n          !str_detect(Question, \"__\"))|&gt;\n  group_by(Round) |&gt;\n  mutate(linenumber = row_number()) |&gt;\n  ungroup() |&gt;\n  unnest_tokens(bigram, Question, token = \"ngrams\", n = 2) |&gt;\n  filter(bigram != \"NA\")\n \n# TF_IDF statistics\njeopardy2word_tf_idf &lt;- jeopardy2word |&gt;\n  count(Round, bigram) |&gt;\n  bind_tf_idf(bigram, Round, n) |&gt;\n  arrange(desc(tf_idf)) \n\n# Graph 3, text analysis 2\njeopardy2word_tf_idf |&gt;\n  group_by(Round) |&gt;\n  arrange(desc(tf_idf)) |&gt;\n  slice_max(tf_idf, n = 10) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = fct_reorder(bigram, tf_idf), y = tf_idf, fill = Round)) +\n    geom_col(show.legend = FALSE) +\n    coord_flip() +\n    facet_wrap(~Round, scales = \"free\")\n\n\n\n\n\n\n\n\nLet’s see what questions “Wanamaker”, “suecica”, or “Hadna came from and what their answers were. We can also look at what the answers were for the most common 2-word question\n\n# see what answers had \"wanamaker\"\njeopardy_tidy |&gt;\n  filter(Round == \"Final Jeopardy!\") |&gt;\n  filter(str_detect(word, \"wanamaker\"))\n\n# A tibble: 2 × 7\n  `Show Number` `Air Date` Round           Category         Value Answer   word \n          &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;           &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;\n1          3026 1997-10-27 Final Jeopardy! FAMOUS BUILDINGS    NA The Glo… wana…\n2          4777 2005-05-17 Final Jeopardy! FAMILIAR PHRASES    NA The cus… wana…\n\n# get the questions\njeopardy |&gt;\n  filter(Answer == \"The customer is always right\" | Answer == \"The Globe Theatre\") |&gt;\n  select(Question)\n\n# A tibble: 2 × 1\n  Question                                                                      \n  &lt;chr&gt;                                                                         \n1 Richard Burbage & Sam Wanamaker, about 400 years apart, were responsible for …\n2 This 5-word rule or maxim has been attributed to both H. Gordon Selfridge & J…\n\n\nFor wanamaker, we can see there are two separate people with the last name wanamaker had questions asked about them\n\njeopardy_tidy |&gt;\n  filter(Round == \"Final Jeopardy!\") |&gt;\n  filter(str_detect(word, \"suecica\"))\n\n# A tibble: 2 × 7\n  `Show Number` `Air Date` Round           Category   Value Answer         word \n          &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;\n1          2984 1997-07-17 Final Jeopardy! SCIENTISTS    NA Carolus Linna… suec…\n2          2984 1997-07-17 Final Jeopardy! SCIENTISTS    NA Carolus Linna… suec…\n\njeopardy |&gt;\n  filter(Answer == \"Carolus Linnaeus\") |&gt;\n  select(Question)\n\n# A tibble: 7 × 1\n  Question                                                                      \n  &lt;chr&gt;                                                                         \n1 \"This N. European said his grave-stone should be inscribed Princeps botanicor…\n2 \"In 1735 this Swed. naturalist published \\\"Systema Naturae\\\", naming & classi…\n3 \"This 18th century Swedish botanist was introduced to botany by his father, a…\n4 \"While in Holland in 1735, this Swede published his \\\"Systema Naturae\\\", a ta…\n5 \"\\\"Kings play chess on finely grained sand\\\" is a mnemonic device used to hel…\n6 \"In the 1740s he published \\\"Flora Suecica\\\" & \\\"Fauna Suecica\\\", 2 volumes o…\n7 \"Because he wrote his books in Latin, Swedish botanist Carl von Linne was bet…\n\n\nHere we can see that there were 7 different jeopardy questions asking about Carolus Linnaeus as an answer, but one Final jeopardy round that had plant publications mentioning them.\n\njeopardy2word |&gt;\n  filter(Round == \"Final Jeopardy!\") |&gt;\n  filter(str_detect(bigram, \"years service\")) # 3rd regular expression\n\n# A tibble: 3 × 8\n  `Show Number` `Air Date` Round         Category Value Answer linenumber bigram\n          &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;       &lt;int&gt; &lt;chr&gt; \n1          4165 2002-10-11 Final Jeopar… FAMOUS … None  Benja…        623 years…\n2          4689 2005-01-13 Final Jeopar… FEDERAL… None  Georg…        864 years…\n3          5025 2006-06-16 Final Jeopar… TRANSPO… None  London        980 years…\n\n\nAs for years service, we see that the answers have famous leaders, representatives, and London which are fairly well known. Let’s see how many questions have Benjamin Franklin as an answer\n\nsum(str_count(jeopardy$Answer, \"Benjamin Franklin\")) # 3rd str_ function\n\n[1] 76\n\n\nThere were 76 questions with Benjamin Franklin as an answer.\n\n\nFinal thoughts\nFrom these graphs and analyses, we can tell that because there are so many Jeopardy! and Double Jeopardy! questions, they have a lot of the same common phrases and ways of asking questions, likely due to subtle tendencies from the writers. Questions from these rounds are seemingly all neutral or positively sentimented over the average of thousands of shows. This could be due to wanting to avoid very negative topics or other harsh language for television. While there definetly are tendencies in Jeopardy questions, there are so many that a good, broad knowledge of many different topics is needed to beat the game and come out on top of opponents."
  },
  {
    "objectID": "MiniProject1.html",
    "href": "MiniProject1.html",
    "title": "Mini Project 1",
    "section": "",
    "text": "Data Source Links :\nState Level Poverty Levels https://data.ers.usda.gov/reports.aspx?ID=4040\nState Level Population https://data.ers.usda.gov/reports.aspx?ID=4049\nHealth Insurance Coverage of the Total Population.\nhttps://www.kff.org/other/state-indicator/total-population/?currentTimeframe=0&sortModel=%7B%22colId%22:%22Location%22,%22sort%22:%22asc%22%7D\n\nlibrary(tidyverse)\nlibrary(maps)\nlibrary(readr)\nlibrary(leaflet)\nlibrary(mapproj)\n\n# Loading Data + changing state to lowercase\npoverty_data &lt;- read_csv(\"Datastat272Miniproject1poverty2.csv\")\nhead(poverty_data)\n\n# A tibble: 6 × 14\n  state   poverty_2023 children_poverty_2023 pop_1990 pop_2000 pop_2010 pop_2020\n  &lt;chr&gt;          &lt;dbl&gt;                 &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Alabama       780043                236320  4040389  4447207  4780118  5024294\n2 Arizona       917925                246285  3665339  5130247  6392292  7157902\n3 Arkans…       459700                140145  2350624  2673293  2916029  3011490\n4 Califo…      4597732               1242302 29811427 33871653 37254522 39538212\n5 Colora…       538673                129751  3294473  4302086  5029319  5773707\n6 Connec…       359550                 92619  3287116  3405650  3574151  3605912\n# ℹ 7 more variables: pop_2023 &lt;dbl&gt;, Employer &lt;dbl&gt;, Above50_Employer &lt;chr&gt;,\n#   Medicaid &lt;dbl&gt;, Medicare &lt;dbl&gt;, Military &lt;dbl&gt;, Uninsured &lt;dbl&gt;\n\npoverty_data &lt;- poverty_data |&gt;\n  mutate(state = str_to_lower(state))\n\nhead(poverty_data)\n\n# A tibble: 6 × 14\n  state   poverty_2023 children_poverty_2023 pop_1990 pop_2000 pop_2010 pop_2020\n  &lt;chr&gt;          &lt;dbl&gt;                 &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 alabama       780043                236320  4040389  4447207  4780118  5024294\n2 arizona       917925                246285  3665339  5130247  6392292  7157902\n3 arkans…       459700                140145  2350624  2673293  2916029  3011490\n4 califo…      4597732               1242302 29811427 33871653 37254522 39538212\n5 colora…       538673                129751  3294473  4302086  5029319  5773707\n6 connec…       359550                 92619  3287116  3405650  3574151  3605912\n# ℹ 7 more variables: pop_2023 &lt;dbl&gt;, Employer &lt;dbl&gt;, Above50_Employer &lt;chr&gt;,\n#   Medicaid &lt;dbl&gt;, Medicare &lt;dbl&gt;, Military &lt;dbl&gt;, Uninsured &lt;dbl&gt;\n\n# Loading us state map data\nus_states &lt;- map_data(\"state\")\nhead(us_states)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n# Loading Shape File State Map Data (taking out unneeded regions)\nlibrary(sf) \nstates &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")\nstates &lt;- states |&gt;\n  filter(!(name %in% c(\"Alaska\", \n                       \"Hawaii\", \n                       \"Puerto Rico\", \n                       \"District of Columbia\"))) |&gt;\n  mutate(name = str_to_lower(name))\n  \nhead(states)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.4108 ymin: 30.2472 xmax: -71.79931 ymax: 42.05\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 4\n  id    name        density                                             geometry\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 01    alabama        94.6 (((-87.3593 35.00118, -85.60667 34.98475, -85.43141…\n2 04    arizona        57.0 (((-109.0425 37.00026, -109.048 31.33163, -111.0744…\n3 05    arkansas       56.4 (((-94.47384 36.50186, -90.15254 36.49638, -90.0649…\n4 06    california    242.  (((-123.2333 42.00619, -122.3789 42.01166, -121.037…\n5 08    colorado       49.3 (((-107.9197 41.00391, -105.729 40.99843, -104.053 …\n6 09    connecticut   739.  (((-73.05353 42.03905, -71.79931 42.02262, -71.7993…\n\n\nNumeric - Graph of Percent of People in Poverty - Static\n\nlibrary(viridis) \npoverty_data |&gt;\n  # joining to us_states\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt;\n  rename(region = state) |&gt;\n  # creating % variable\n  mutate(percent_poverty = (poverty_2023/pop_2023)) |&gt;\n  # graph\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = percent_poverty), color = \"black\") + \n  labs(fill = \"Percent of People in Poverty\",\n       title = \"Poverty Map of the U.S.\") +\n  coord_map() +  \n  theme_void() +\n  scale_fill_viridis()  # color scale\n\n\n\n\n\n\n\n\nThis is a graph of the Contiguous United States colored by the amount of the state comprising of people who are below the poverty line. The data is per state, ranging from 0.075 to 0.175 and the colors on the map ranging from a dark blue/purple to a bright yellow. We can see that many southern states in the Southeast have high poverty rates, particularly Louisiana and Mississippi. The lower poverty states are more in the north east (Vermont, New Hampshire) or the midwest/west like from Minnesota to Utah.\nCategorical - Above/below 50% of people have Employer Health Insurance - Static\n\npoverty_data |&gt;\n  # Join states map to data\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt;\n  rename(region = state) |&gt;\n  # Fraphing data onto map\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = Above50_Employer), color = \"darkgrey\", linewidth = 0.2) + \n  labs(fill = \"Yes/No\",\n       title = \"Do &gt;50% of people have Employer Health Insurance?\") +\n  coord_map() + \n  theme_void() +  \n  # Color Scale similar to ends on Previous graph\n  scale_fill_manual(values = c(\"yellow\", \"darkblue\"))\n\n\n\n\n\n\n\n\nThis is a map of the Contiguous United States colored in two categories by if over half of people in the state have health insurance that comes from their job or Employer. States Labeled in Blue do have over 50% with employer health insurance and states labeled in yellow have below 50%. We can see a similar trend to the previous static graph that many of the southern states have below 50% of people with employer health insurance while midwest/west areas with low poverty (from Minnesota to Utah, and a few in the Northeast) have over 50% with employer health insurance. Not having your employer pay for Health insurance means that you must get health insurance yourself, potentially having to pay much more than you would if the employer included it in benefits.\nNumeric - Graph of % of people in Poverty - Interactive\n\n# Making percent_poverty permanent and rounding\npoverty_data &lt;- poverty_data |&gt;\n  mutate(percent_poverty = (poverty_2023/pop_2023)*100) |&gt;\n  mutate(percent_poverty = round(percent_poverty, digits = 2))\n\n# Creating category bins and assigning color palette\nbins &lt;- c(5, 7.5, 10.0, 12.5, 15.0, 17.5, Inf)\npal &lt;- colorBin(\"YlOrRd\", domain = poverty_data$percent_poverty, bins = bins)\n\n# Creating labels on hover\nlibrary(htmltools)\nlibrary(glue)\n\n# Modifying Labels to look good\npoverty_data &lt;- poverty_data |&gt;\n  mutate(state = str_to_title(state)) |&gt;\n  mutate(labels = str_c(state, \n                        \": \", \n                        percent_poverty, \n                        \"% of people are in Poverty\")) \n\n# creating a labels variable\nlabels &lt;- lapply(poverty_data$labels, HTML)\n\n\n# Interactive Graph\nleaflet(states) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    fillColor = ~pal(poverty_data$percent_poverty),\n    weight = 1,\n    opacity = 5,\n    color = \"white\",\n    dashArray = \"1\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 1,\n      color = \"#555\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"2px 2px\"),\n      textsize = \"10px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = pal, values = ~poverty_data$percent_poverty, opacity = 0.7, title = \"% of People in Poverty\",\n    position = \"bottomright\")\n\n\n\n\n\nThis is a graph of the Contiguous United States which is filled with color based on the % of people in poverty by state. The states range from under 7.5% of people in poverty to over 17.5% of people in poverty. Light yellow colors indicate a lower % in poverty while darker red colors indicate a higher percent of people in poverty. There is a trend that particularly south and southeastern states have higher rates of poverty than the rest of the United States with Louisiana and Mississippi being the highest. States in the northeast like New Hampshire and some across the midwest/west from Minnesota to Utah have lower poverty levels.\nCategorical - Above/below 50% of people have Employer Health Insurance\n\nEmployer_Insurance_data &lt;- read_csv(\"Datastat272Miniproject1poverty2.csv\")\nEmployer_Insurance_data\n\n# A tibble: 48 × 14\n   state  poverty_2023 children_poverty_2023 pop_1990 pop_2000 pop_2010 pop_2020\n   &lt;chr&gt;         &lt;dbl&gt;                 &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 Alaba…       780043                236320  4040389  4447207  4780118  5024294\n 2 Arizo…       917925                246285  3665339  5130247  6392292  7157902\n 3 Arkan…       459700                140145  2350624  2673293  2916029  3011490\n 4 Calif…      4597732               1242302 29811427 33871653 37254522 39538212\n 5 Color…       538673                129751  3294473  4302086  5029319  5773707\n 6 Conne…       359550                 92619  3287116  3405650  3574151  3605912\n 7 Delaw…       109203                 32159   666168   783559   897947   989946\n 8 Flori…      2741486                689967 12938071 15982571 18804589 21538216\n 9 Georg…      1477354                469680  6478149  8186653  9688737 10713771\n10 Idaho        193938                 52105  1006734  1293957  1567658  1839117\n# ℹ 38 more rows\n# ℹ 7 more variables: pop_2023 &lt;dbl&gt;, Employer &lt;dbl&gt;, Above50_Employer &lt;chr&gt;,\n#   Medicaid &lt;dbl&gt;, Medicare &lt;dbl&gt;, Military &lt;dbl&gt;, Uninsured &lt;dbl&gt;\n\nEmployer_Insurance_data &lt;- Employer_Insurance_data |&gt;\n  mutate(state = str_to_lower(state))\n\n# Creating color palette for categoricals\npal &lt;- colorFactor(c(\"red\", \"yellow\"),\n  domain = Employer_Insurance_data$Above50_Employer)\n\n# Creating labels on hover\nEmployer_Insurance_data &lt;- Employer_Insurance_data |&gt;\n  mutate(state = str_to_title(state)) |&gt;\n  mutate(labels = str_c(state, \n                        \": \", \n                        Employer*100,\n                        \"% of people have Employer Insurance\")) \n\n# creating a labels variable\nlabels &lt;- lapply(Employer_Insurance_data$labels, HTML)\n\n\n# Interactive Graph\nleaflet(states) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    fillColor = ~pal(Employer_Insurance_data$Above50_Employer),\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"1\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 1,\n      color = \"#555\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = pal, values = ~Employer_Insurance_data$Above50_Employer, opacity = 0.7, title = \"Greater than 50% Employer Insurance\",\n    position = \"bottomright\")\n\n\n\n\n\nThis is a graph of the Contiguous United States filled in based on if the states have greater or less than 50% of people who have Employer Health Insurance. A red color means that states have less than 50% of people with Employer Health Insurance while yellow represents states with greater than 50%. Again, we see that southern states particularly have low rates of Employer health Insurance while states in the midwest/west (Minnesota to Utah) and northeastern (New hampshire, Massachussets) regions have greater than 50% of people with Employer Health Insurance. For many people, having employers cover health insurance means that they will have to spend less out of their own pocket and time figuring out what insurance to get. This leads to people having less money overall and potentially going into poverty based on healthcare costs."
  },
  {
    "objectID": "GlobalSemester.html",
    "href": "GlobalSemester.html",
    "title": "Global Semester",
    "section": "",
    "text": "In fall of 2024, I went on Global Semester. Here is a recap of the entire program and purpose in each country."
  },
  {
    "objectID": "GlobalSemester.html#what-is-global-semester",
    "href": "GlobalSemester.html#what-is-global-semester",
    "title": "Global Semester",
    "section": "What is Global Semester?",
    "text": "What is Global Semester?\nGlobal Semester is an annual fall study abroad program at St Olaf College providing twenty students with opportunities to develop insight into cultures around the world and their social, political, and economic contexts through the combination of course structure, experiential learning activities, and engagement with communities.\nFollow our Instagram!!! @gallivantwithglobal Follow our TikTok! @global.gallivant\nDuring Global Semester, 20 students take one course in each country, as well as an overarching course taught by St Olaf faculty."
  },
  {
    "objectID": "GlobalSemester.html#coursework",
    "href": "GlobalSemester.html#coursework",
    "title": "Global Semester",
    "section": "Coursework",
    "text": "Coursework\n\nThe Ethics of Study and Service Abroad\nIn The Ethics of Study and Service Abroad we will explore the complexities and ethical dilemmas related to studying abroad, global experiential learning, and the desire to serve people abroad. Throughout our extended stays in each country, our study will be guided by questions like the following: When studying abroad, how can we learn from or with other people and places in the world in an ethical, non-exploitative way? How can service be an ethical part of study abroad? What ethical issues are present in teaching abroad contexts?\n\n\nEnvironmental Sustainability in Costa Rica\nThis course will use the United Nations Sustainable Development Goals as a critical lens for examining sustainable development in tropical regions, using the case study of Costa Rica—a country renowned for its leadership in land protection, reforestation, high educational attainment, and progress toward gender and racial equality. Costa Rica has been a global leader in conserving its lands/water systems and is emerging as a leader in addressing how to ensure sustainable development and management systems. This program will immerse you in efforts to create a green, circular, and sustainable economy in Costa Rica.\n\n\nPolitical and Social Change in Southern Africa: Namibia\nThis course examines the legacy of colonialism and apartheid in Southern Africa. The course will review the political history of Southern Africa with a particular focus on South Africa and Namibia as countries who have endured years of racial segregation under the apartheid system. It will highlight the social and political movements that have evolved in the struggle for independence.\n\n\nEconomic Reform and Development in Vietnam\nThis course charts Vietnam’s transition from a centrally planned to a market-oriented economy and the associated socio-political changes as well as environmental costs. Students examine in detail the major turning point in Vietnam’s economic development the Doi Moi “Renovation” economic reforms launched in 1986 – and the rapid growth in trade and investment since that time. The course additionally examines development ethics and equity issues within the context of the country’s growing market economy and the urban consumer class."
  },
  {
    "objectID": "GlobalSemester.html#my-blog-post",
    "href": "GlobalSemester.html#my-blog-post",
    "title": "Global Semester",
    "section": "My Blog Post",
    "text": "My Blog Post\nHere is the blog post I wrote for my main assignment during Global Semester. This was during our excursion in Swakopmund, Namibia\n\n\n\nSand Sign in Namibia\n\n\nHello, welcome to the blog. This week was busy, so if you don’t have time, here’s a TL;DR: We heard from several speakers on the historical context of Namibia, including the Namibian Genocide, drove around Walvis Bay and Swakopmund with lots of sand dunes, and met wild seals face-to-face. There was lots to talk about this week!\nOn Wednesday we started with a tour of the Namibian Parliament building. Parliament here is similar to the U.S. in that two bodies determine the laws: the National Assembly and National Council. The assembly is the main lawmaking body, while the council reviews laws and sends back recommendations of what to fix, which the assembly doesn’t even have to listen to. Parliament here strongly focuses on youth politics, with the youngest member presiding and many other related organizations like the National Youth Council.\n\n\n\nNamibian Flag at Parliament\n\n\nIn the afternoon, we started learning about the Namibian Genocide with a documentary and a discussion with Dr. Elison Tjirera. Most of us had never even heard about the events that occurred here and were shocked at what atrocities occurred. The German army and leadership were focused on expansion in the early 1900s and would do anything to claim more land. They forced the local Nama and OvaHerero tribes into a desert and kept them starving without food or water. Later, they made concentration camps for the people that remained. We also discussed how Swakopmund is now a “little Germany,” which I’ll discuss later in the blog.\nBefore we headed to Swakopmund, we had two more speakers- JP Van de Westhuizen and Shane from the Namibian Ministry of Environment, Forestry, and Tourism. We learned from JP that the core of the genocide was the need for land, reiterating what we heard in the documentary. Today, land is split very unequally in Namibia. About half of the land in Namibia is privately owned, and of that half, 70% is owned by white people. This problem isn’t being fixed soon as the leading party, SWAPO, is very slow at land resolution, having only 25 completed resolutions out of 176 needed over the last 6 years.\nAs for hunting, it gave us a new understanding of game hunting in terms of a sustainable way of doing it. Namibia has systems of expensive permits with many years of training to legally hunt big game like elephants. Poaching does occur, which is hard for patrollers as they cover over a million hectares of land with only 14 guards patrolling.\n\nSwakopmund\n\n\n\nDrive to Swakopmund\n\n\nThe next day we headed to Swakopmund! Finally, a place without 5,000+ feet of elevation. I’ve never appreciated oxygen-rich air and the ocean breeze as much.\nWe enjoyed lunch by the coast, climbed some rocks, and made seagull sounds before we heard from our next guest speaker and tour guide, Laidlaw Peringanda, with a historical tour of Swakopmund.\n\n\n\nSwakopmund Coast\n\n\nAfter watching the documentary earlier in the week, I imagined a city from the far past with old monuments symbolizing oppression and powerful regimes. In Swakopmund, they were standing right in the center of the town. We saw monuments to the dead German soldiers during the genocide, German statues commemorating the soldiers that died during WW1 and WW2, and heard about stores that sold Third Reich merchandise down the street.\nNext, we went to the Swakopmund cemetery, which scared me about how different the graves of the Nama and OvaHerero tribes were treated to those of the Germans. You can see in the pictures below that the German graves are surrounded by lush greenery, fancy engravings, and a sign that translates to “They gave their lives for you.” The graves here are looked after by the local council and cleaned often.\n\n\n\nGerman Grave Site in Swakopmund\n\n\nOn the other side (in the same cemetery not even 200 feet away), you see the Nama & OvaHerero Genocide monuments with sand mounds in the background. These mounds are the mostly beheaded remains of those killed in concentration camps from hunger, intense labor, and more at the hands of German soldiers. Bodies were beheaded so that the Germans could study skull anatomy differences. The Lenz Company’s concentration camp alone (yes, companies owned labor camps for free labor) reported over 1300 deaths due to physical exhaustion.\n\n\n\nOvaHerero Grave\n\n\nThese graves are unkept by authorities, leaving relatives and volunteers to restore the graves every 4 months just to avoid body parts surfacing from the sands. Many of these volunteers get nightmares and trauma from this service. Additionally, the sign of the monument was only changed in 2020 (4 years ago!!!) to change the cause of mass death from “mysterious circumstances” to dying of starvation, labor, sexual abuse, disease, fatigue, and adverse weather conditions.\nEven worse, the government doesn’t care. Housing developments have encroached on the cemetery. During remembrance services for the genocide, local officials were seen going back to German graves to pay their respects. Even street sign changes that removed the names of old important German people have been branded on buildings to maintain the names in the community. It simply is an issue around little international pressure to pay reparations or recognize the atrocities they committed since it is merely Germany vs Namibia. As Dr. Tjirera put it earlier, the Germans don’t believe their lives are meaningful enough to care.\nOn Saturday, we headed to Walvis Bay to talk with a member of the Municipal Council about housing and development issues within the town. He stressed that Namibia had little to export and had to import most of its resources, with Walvis Bay being the country’s main port city. Housing development resources were scarce, leaving many people without homes. We saw this with metal shacks built in backyards homeowners use as “guesthouses” that people rent for decades.\nHe also discussed the development issue surrounding large malls taking business from the central business district. As an economics major going into a commercial real estate field, it was interesting to connect my experience in a place without U.S. laws to how life is for people without large apartments or tall developments. Apartments here form around a central “resource” building, with small homes surrounding it.\nAfter a long few days of learning, we had to end it with some fun outdoor activities that took up the rest of the weekend. We got to climb Dune 7, look out over several sand dunes into the ocean, and recreate the dune movies, making Paul our new Paul Atreides. We had to climb up the entire dune using our hands and feet, making every part of our body sore. It was all worth it at the top with several amazing photos and the chance to jump and run back down the dune.\n\n\n\nDune Buggies\n\n\nWe also got to go dune buggying back in Swakopmund! We got our own dune buggies and followed in a line cruising over and around the dunes. We all got to go super fast and feel the wind rush by us as sand hit our visors.\nSunday was the best day yet. When we first got to Namibia, we were surprised with a kayaking trip with Catamaran Cruises, but we didn’t know about the seal and pelican tour that came with it!\n\n\n\nPelican\n\n\nFrom the moment we started, seals were climbing on the dock and coming inches away from all of us. They were scary initially, but we learned they just wanted the fish the guides fed them. Some of us even got to feed the seals and got attacked by pelicans.\n\n\n\nSeals on the boat\n\n\nWhen we got to the other side of the bay, we started kayaking. Paul and I were a great duo, beating almost every pair of Globies in races (Fiona and Eva won every time).\nAfter that, we enjoyed a great lunch on the bay, and a small group of us went camel riding. Most of us had never ridden horses before, so we didn’t know any technique when riding them (it hurt a lot). Luckily, Fiona helped us with some technique since she did some horse-riding before.\n\n\n\nThe group kayaking\n\n\nThanks for reading! We’ve been on global for so long and still have so much left to learn and do, but it feels like it is going by so fast. It’s good since we are all enjoying ourselves, but I hope our second half doesn’t go by too fast.\nYahya chouhada,\nJean-Luc"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Campus Activities",
    "section": "",
    "text": "Here is what I am involved in on Campus and Life!"
  },
  {
    "objectID": "about.html#u.s.-bank-specialty-credit-analyst",
    "href": "about.html#u.s.-bank-specialty-credit-analyst",
    "title": "Campus Activities",
    "section": "U.S. Bank Specialty Credit Analyst",
    "text": "U.S. Bank Specialty Credit Analyst\nCurrently I am working part-time for U.S. Bank in their Specialty CRE department. This sector deals with financial analysis of large corporate companies that invest in Real Estate properties. These companies are designated as REITs (Real Estate Investment Trusts), REOCs (Real Estate Operating Companies), and Mutual Funds. We conduct quarterly analyses of legal agreements to underwrite their ability to pay off loans"
  },
  {
    "objectID": "about.html#norseman-band",
    "href": "about.html#norseman-band",
    "title": "Campus Activities",
    "section": "Norseman Band",
    "text": "Norseman Band\nNorseman Band is St. Olaf’s second performing concert band and I play percussion in the ensemble. I was treasurer of the band in my Junior year but had to step down due to my study abroad in the fall. The band consists of mostly sophomores and freshman but gives a space for many non-major musicians to practice for fun.\nHere is a link for our last concert. Our next concert is on May 3rd at 7:30 PM in Boe Chapel!"
  },
  {
    "objectID": "about.html#sga-deputy-secretary-of-finance",
    "href": "about.html#sga-deputy-secretary-of-finance",
    "title": "Campus Activities",
    "section": "SGA Deputy Secretary of Finance",
    "text": "SGA Deputy Secretary of Finance\nIn Senior year, I got appointed as the deputy secretary of finance for two SGA branches. My role is to help plan events, keep budgets, and help the organization reconcile credit card transactions.\n\nInvolvement Board\nInvolvement Board is the main branch that gives funding to clubs on campus so that they can plan events, fund large costs, and make St. Olaf a fun place to enjoy with friends. We recently had a scavenger hunt where clubs went all over campus to find things like the turtle shell on holland or the Giant Green sunglasses in CAD. We asked clubs to take photos of all the wooden doors on campus and 555 photos were submitted!\n\n\nStudent Affairs Committee\nStudent Affairs is the main way that students communicate with the Board of Regents. Our main role is to serve as a student voice that advocates to the BoR in a concise manner. We also do student studies to see if their concerns are being heard and what things we can do to make St. Olaf a better place. Some of our initiatives include the textbook project and the new swingset coming to campus!"
  },
  {
    "objectID": "about.html#vice-president-of-finance-club",
    "href": "about.html#vice-president-of-finance-club",
    "title": "Campus Activities",
    "section": "Vice President of Finance Club",
    "text": "Vice President of Finance Club\nFor Finance Club, we give a space for students with a finance emphasis in economics to explore careers, learn about events in finance, and overall try to understand what it means to be in a finance career. I have explored multiple roles in Banking and have been working for U.S. Bank in various credit-related fields for the last 3 summers."
  },
  {
    "objectID": "about.html#treasurer-of-board-game-club",
    "href": "about.html#treasurer-of-board-game-club",
    "title": "Campus Activities",
    "section": "Treasurer of Board Game Club",
    "text": "Treasurer of Board Game Club\nI am a huge fan of board games and I have led a club that meets every week for the past 4 years just to play board games! We average about 25 members each week (ranging from 20 to 40 on a good week) and bring almost 100 games each week to pick from. My favorites are Monopoly (even though I got banned from playing it), One night werewolf, and Ticket to Ride."
  },
  {
    "objectID": "about.html#ole-cup",
    "href": "about.html#ole-cup",
    "title": "Campus Activities",
    "section": "2024 Ole Cup",
    "text": "2024 Ole Cup\nIn spring of 2024 I participated in the Ole cup with some friends I created a venture with in New Zealand. We didn’t win any money, but we had a great time creating a potential business together and learned how entrepreneurship works in practice\nHere is a link to the 2024 Ole Cup. Click through to watch me present!"
  },
  {
    "objectID": "about.html#reading",
    "href": "about.html#reading",
    "title": "Campus Activities",
    "section": "Reading",
    "text": "Reading\nLately I have been trying to read a lot more, especially at night before bed. Some of the books I have read this year are the new Hunger Games book, the first 4 books of the Harry Potter Series, Flowers for Algernon, and Atomic Habits."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jean-LucCollette.github.io",
    "section": "",
    "text": "Senior Quantitative Economics Major - Statistics and Data Science Concentration, Finance Emphasis\nU.S. Bank Specialty Credit Analyst\n\n\nBA in Quantitative Economics, Concentration in Statistics and Data Science with a Finance emphasis\n2025 | St. Olaf College\nHigh School Diploma 2021 | Edina High School\n\n\n\nPhone: 763-453-9730\nEmail: collet3@stolaf.edu\nAddress: 6704 Rosemary Lane, Edina, MN 55439."
  },
  {
    "objectID": "index.html#jean-luc-collette",
    "href": "index.html#jean-luc-collette",
    "title": "Jean-LucCollette.github.io",
    "section": "",
    "text": "Senior Quantitative Economics Major - Statistics and Data Science Concentration, Finance Emphasis\nU.S. Bank Specialty Credit Analyst\n\n\nBA in Quantitative Economics, Concentration in Statistics and Data Science with a Finance emphasis\n2025 | St. Olaf College\nHigh School Diploma 2021 | Edina High School\n\n\n\nPhone: 763-453-9730\nEmail: collet3@stolaf.edu\nAddress: 6704 Rosemary Lane, Edina, MN 55439."
  },
  {
    "objectID": "Miniproject2.html",
    "href": "Miniproject2.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "Data Sources: https://www.sports-reference.com/cbb/seasons/men/1993-advanced-school-stats.html https://www.sports-reference.com/cbb/seasons/men/2024.html\nMotivation: For our project, we used data from the Sports Reference website. March Madness refers to the annual NCAA college basketball tournament which typically happens in March. With “March Madness” being a popular event that attracts the excitement of sports fans, and can potentially line the pockets of many gamblers, it serves as an interesting exercise to analyze the performance of the different teams who participate. We are compiling a dataset of both the conference performance and individual school performance.\nIn order to do this, we used the rvest and polite package in order to scrape the contents of the site and build our dataset. Before scraping however, we verified using the paths_allowed function to check the robots.txt which gave permission for scraping. We created functions in order to scrape the site over multiple years in order to gain a broader understanding of how performance has changed over time. Our analysis may provide insights into what teams may be standout picks based on previous performance and may highlight strengths and weaknesses that lie in a team alongside variables that affect their performance. We could also analyse what conferences have historically been the best and what that may mean with many conferences changing in recent years.\n\nIndividual School Performance\n\n# check that scraping is allowed\nrobotstxt::paths_allowed(\"https://www.sports-reference.com/cbb/seasons/men/1993-advanced-school-stats.html\")\n\n\n www.sports-reference.com                      \n\n\n[1] TRUE\n\n# Step 1: Download the HTML and turn it into an XML file with read_html()\nbasketball_site &lt;- read_html(\"https://www.sports-reference.com/cbb/seasons/men/1993-advanced-school-stats.html\")\n\n\n#Main function to create list\nbasketball_stats &lt;- function(year_list) {\n  \n basketball_stats_data_list &lt;- purrr::map(year_list, ~ basketball_scrape(year = .x) %&gt;% mutate(year = .x))\n \nbasketball_stats_all_years &lt;- list_rbind(basketball_stats_data_list)\n\nreturn(basketball_stats_all_years)\n\n}\n\n\n#Smaller function to grab data \n\nbasketball_scrape &lt;- function(year) {\n  \n  session &lt;- bow(str_c(\"https://www.sports-reference.com/cbb/seasons/\", year,\"-advanced-school-stats.html\"), force = TRUE)\n  \ntitle_temp &lt;- html_nodes(basketball_site, css = \"table\") #Look for tables\n\n#The table of interest is the first one so we use [[1]]\nBasketball_table &lt;- html_table(title_temp, header = T, fill = T)[[1]] %&gt;%  row_to_names(row_number = 1) %&gt;% \n  clean_names() %&gt;%  \n  select(-c(starts_with(\"na\"))) #the table has na cols so we clean names then drop those cols\n\nreturn(Basketball_table)\n}\n\ntest&lt;- basketball_scrape(\"2000\") # test data for the smaller function\n\nWarning: Row 1 does not provide unique names. Consider running clean_names()\nafter row_to_names().\n\ntest\n\n# A tibble: 326 × 29\n   rk    school      g     w     l     w_l_percent srs   sos   w_2   l_2   w_3  \n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 1     Air Force   28    9     19    .321        -7.45 2.05  3     15    7    \n 2 2     Akron       26    8     18    .308        -10.… -5.06 3     15    5    \n 3 3     Alabama     29    16    13    .552        9.66  7.83  7     9     12   \n 4 4     Alabama St… 27    14    13    .519        -8.49 -9.70 9     5     9    \n 5 5     Alcorn Sta… 27    7     20    .259        -11.… -4.58 5     9     3    \n 6 6     American    28    11    17    .393        -0.17 1.15  6     8     6    \n 7 7     Appalachia… 28    13    15    .464        -6.09 -4.27 8     10    8    \n 8 8     Arizona NC… 28    24    4     .857        20.50 8.28  17    1     14   \n 9 9     Arizona St… 28    18    10    .643        9.24  6.99  11    7     13   \n10 10    Arkansas N… 31    22    9     .710        17.72 8.30  10    6     12   \n# ℹ 316 more rows\n# ℹ 18 more variables: l_3 &lt;chr&gt;, w_4 &lt;chr&gt;, l_4 &lt;chr&gt;, tm &lt;chr&gt;, opp &lt;chr&gt;,\n#   pace &lt;chr&gt;, o_rtg &lt;chr&gt;, f_tr &lt;chr&gt;, x3p_ar &lt;chr&gt;, ts_percent &lt;chr&gt;,\n#   trb_percent &lt;chr&gt;, ast_percent &lt;chr&gt;, stl_percent &lt;chr&gt;, blk_percent &lt;chr&gt;,\n#   e_fg_percent &lt;chr&gt;, tov_percent &lt;chr&gt;, orb_percent &lt;chr&gt;, ft_fga &lt;chr&gt;\n\nyears&lt;- c(\"2002\",\"2001\")\ntest2 &lt;- basketball_stats(years) #test data for larger function\n\nWarning: Row 1 does not provide unique names. Consider running clean_names() after row_to_names().\nRow 1 does not provide unique names. Consider running clean_names() after row_to_names().\n\n\n\n\nConference Level performance\n\n# check that scraping is allowed\nrobotstxt::paths_allowed(\"https://www.sports-reference.com/cbb/seasons/men/2024.html\")\n\n\n www.sports-reference.com                      \n\n\n[1] TRUE\n\n# 1 - read as an html\nnih &lt;- read_html(\"https://www.sports-reference.com/cbb/seasons/men/2024.html\")\n\n# 2 - extract the table\ntitle_temp &lt;- html_nodes(nih, css = \"table\")\ntitle_temp\n\n{xml_nodeset (2)}\n[1] &lt;table class=\"sortable stats_table\" id=\"conference-summary\" data-cols-to- ...\n[2] &lt;table class=\"poll sortable stats_table\" id=\"coaches-polls\" data-cols-to- ...\n\n# 3: html_table() - making a table and finding the right one\nhtml_table(title_temp, header = TRUE, fill = TRUE)    # find the right table\n\n[[1]]\n# A tibble: 33 × 13\n      Rk Conference       Schls     W     L `W-L%`   SRS   SOS    AP  NCAA    FF\n   &lt;int&gt; &lt;chr&gt;            &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1 Pac-12 Conferen…    12   226   181  0.555 10.9   8.54     1     4     0\n 2     2 Southeastern Co…    14   284   197  0.59  12.2   8.38     1     8     1\n 3     3 Big Ten Confere…    14   284   199  0.588 13.0   9.15     1     6     1\n 4     4 Big 12 Conferen…    14   295   184  0.616 15.1   9.12     1     8     0\n 5     5 West Coast Conf…     9   157   141  0.527  1.8   1.35     1     2     0\n 6     6 Big East Confer…    11   219   163  0.573 12.8   9.58     0     3     1\n 7     7 Atlantic Coast …    15   298   218  0.578 11.4   8.31     0     5     1\n 8     8 Sun Belt Confer…    14   230   231  0.499 -3.41 -2.42     0     1     0\n 9     9 American Athlet…    14   251   210  0.544  3.68  2.28     0     2     0\n10    10 Atlantic 10 Con…    15   285   215  0.57   4.52  2.55     0     2     0\n# ℹ 23 more rows\n# ℹ 2 more variables: `Regular Season Champ` &lt;chr&gt;, `Tournament Champ` &lt;chr&gt;\n\n[[2]]\n# A tibble: 57 × 23\n   ``          ``    `Week Poll` `Week Poll` `Week Poll` `Week Poll` `Week Poll`\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      \n 1 \"\"          \"\"    \"1\"         \"2\"         \"3\"         \"4\"         \"5\"        \n 2 \"School\"    \"Con… \"Pre\"       \"11/13\"     \"11/20\"     \"11/27\"     \"12/4\"     \n 3 \"UConn\"     \"Big… \"5\"         \"4\"         \"4\"         \"4\"         \"5\"        \n 4 \"Purdue\"    \"Big… \"2\"         \"2\"         \"2\"         \"1\"         \"4\"        \n 5 \"Houston\"   \"Big… \"6\"         \"6\"         \"6\"         \"5\"         \"2\"        \n 6 \"Alabama\"   \"SEC\" \"24\"        \"22\"        \"15\"        \"20\"        \"\"         \n 7 \"Tennessee\" \"SEC\" \"10\"        \"8\"         \"8\"         \"11\"        \"13\"       \n 8 \"UNC\"       \"ACC\" \"21\"        \"18\"        \"14\"        \"16\"        \"10\"       \n 9 \"Illinois\"  \"Big… \"\"          \"23\"        \"22\"        \"24\"        \"18\"       \n10 \"Iowa Stat… \"Big… \"\"          \"\"          \"\"          \"\"          \"\"         \n# ℹ 47 more rows\n# ℹ 16 more variables: `Week Poll` &lt;chr&gt;, `Week Poll` &lt;chr&gt;, `Week Poll` &lt;chr&gt;,\n#   `Week Poll` &lt;chr&gt;, `Week Poll` &lt;chr&gt;, `Week Poll` &lt;chr&gt;, `Week Poll` &lt;chr&gt;,\n#   `Week Poll` &lt;chr&gt;, `Week Poll` &lt;chr&gt;, `Week Poll` &lt;chr&gt;, `Week Poll` &lt;chr&gt;,\n#   `Week Poll` &lt;chr&gt;, `Week Poll` &lt;chr&gt;, `Week Poll` &lt;chr&gt;, `Week Poll` &lt;chr&gt;,\n#   `Week Poll` &lt;chr&gt;\n\nBasketball_table &lt;- html_table(title_temp, header = TRUE, fill = TRUE)[[1]]\nBasketball_table &lt;- Basketball_table |&gt;\n  mutate(year = 2025)\nBasketball_table\n\n# A tibble: 33 × 14\n      Rk Conference       Schls     W     L `W-L%`   SRS   SOS    AP  NCAA    FF\n   &lt;int&gt; &lt;chr&gt;            &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1 Pac-12 Conferen…    12   226   181  0.555 10.9   8.54     1     4     0\n 2     2 Southeastern Co…    14   284   197  0.59  12.2   8.38     1     8     1\n 3     3 Big Ten Confere…    14   284   199  0.588 13.0   9.15     1     6     1\n 4     4 Big 12 Conferen…    14   295   184  0.616 15.1   9.12     1     8     0\n 5     5 West Coast Conf…     9   157   141  0.527  1.8   1.35     1     2     0\n 6     6 Big East Confer…    11   219   163  0.573 12.8   9.58     0     3     1\n 7     7 Atlantic Coast …    15   298   218  0.578 11.4   8.31     0     5     1\n 8     8 Sun Belt Confer…    14   230   231  0.499 -3.41 -2.42     0     1     0\n 9     9 American Athlet…    14   251   210  0.544  3.68  2.28     0     2     0\n10    10 Atlantic 10 Con…    15   285   215  0.57   4.52  2.55     0     2     0\n# ℹ 23 more rows\n# ℹ 3 more variables: `Regular Season Champ` &lt;chr&gt;, `Tournament Champ` &lt;chr&gt;,\n#   year &lt;dbl&gt;\n\n\n\n# function for a given year conference statistics\nbasketball_scrape_c &lt;- function(year) {\n  \n  url &lt;- str_c(\"https://www.sports-reference.com/cbb/seasons/men/\", year, \".html\")\n  robotstxt::paths_allowed(url) # test to ensure it is fine to scrape\n  \n  nih &lt;- read_html(url)\n  \n  title_temp &lt;- html_nodes(nih, css = \"table\")\n  \n  Basketball_table &lt;- html_table(title_temp, header = TRUE, fill = TRUE)[[1]] # selecting the table we want\n  Basketball_table &lt;- Basketball_table |&gt;\n  mutate(year = year) # adding year as a column\n  \nBasketball_table\n\n}\n\ntest3 &lt;- basketball_scrape_c(2022)\n\n\n www.sports-reference.com                      \n\ntest3\n\n# A tibble: 32 × 14\n      Rk Conference       Schls     W     L `W-L%`   SRS   SOS    AP  NCAA    FF\n   &lt;int&gt; &lt;chr&gt;            &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1 Pac-12 Conferen…    12   222   178  0.555  9.49  7.35     1     3     0\n 2     2 Big 12 Conferen…    10   217   126  0.633 15.5   9.75     1     6     1\n 3     3 Big Ten Confere…    14   271   193  0.584 11.9   8.6      1     9     0\n 4     4 Southeastern Co…    14   283   196  0.591 12.3   8.45     0     6     0\n 5     5 Conference USA      14   250   214  0.539 -0.35 -0.62     0     1     0\n 6     6 Atlantic 10 Con…    14   243   208  0.539  3.46  2.3      0     2     0\n 7     7 West Coast Conf…    10   183   141  0.565  5.14  3.26     0     3     0\n 8     8 American Athlet…    11   194   151  0.562  7.32  4.95     0     2     0\n 9     9 Southern Confer…    10   177   148  0.545 -1.71 -1.51     0     1     0\n10    10 Missouri Valley…    10   170   153  0.526  2.66  1.53     0     1     0\n# ℹ 22 more rows\n# ℹ 3 more variables: `Regular Season Champ` &lt;chr&gt;, `Tournament Champ` &lt;chr&gt;,\n#   year &lt;dbl&gt;\n\n# Function for multiple years in data\n\nconference_years &lt;- function(year_list) {\n  \n conference_data_list &lt;- purrr::map(year_list, ~ basketball_scrape_c(year = .x) %&gt;% mutate(year = .x))\n \nconference_stats_all_years &lt;- list_rbind(conference_data_list)\n\nconference_stats_all_years\n\n}\n\n\nyears &lt;- c(\"2018\",\"2019\") # testing the code for multiple years\ntest4 &lt;- conference_years(years) # testing the new function\n\n\n www.sports-reference.com                      \n\n\n www.sports-reference.com                      \n\ntest4\n\n# A tibble: 64 × 14\n      Rk Conference       Schls     W     L `W-L%`   SRS   SOS    AP  NCAA    FF\n   &lt;int&gt; &lt;chr&gt;            &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1 Big East Confer…    10   213   128  0.625 14.2   9.28     0     6     1\n 2     2 Pac-12 Conferen…    12   231   172  0.573  8.61  6.15     1     3     0\n 3     3 Big 12 Conferen…    10   220   130  0.629 15.2   9.83     0     7     1\n 4     4 Southeastern Co…    14   286   193  0.597 13     9.34     0     8     0\n 5     5 Atlantic 10 Con…    14   232   224  0.509  2.26  1.99     0     3     0\n 6     6 Atlantic Coast …    15   315   200  0.612 13.4   8.51     0     9     0\n 7     7 Big Ten Confere…    14   289   189  0.605 13.0   7.99     0     4     1\n 8     8 West Coast Conf…    10   180   159  0.531  1.83  1.43     0     1     0\n 9     9 Conference USA      14   245   221  0.526  0.22  0.2      0     1     0\n10    10 American Athlet…    12   224   168  0.571  5.58  3.41     0     3     0\n# ℹ 54 more rows\n# ℹ 3 more variables: `Regular Season Champ` &lt;chr&gt;, `Tournament Champ` &lt;chr&gt;,\n#   year &lt;chr&gt;"
  }
]